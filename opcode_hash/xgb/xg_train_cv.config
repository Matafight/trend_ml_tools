[xg_conf]
# DO NOT DELET OR ADD ANY PARAMETERS HERE. IF YOU HAVE TO, PLEASE REVISE THE CODE: xg_train_cv.py

# ==========   General Parameters, see comment for each definition  ===========
# choose the booster, can be gbtree or gblinear
booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1


# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 0.3
# minimum loss reduction required to make a further partition
gamma = 0.0
# minimum sum of instance weight(hessian) needed in a child
max_delta_step = 0
colsample_bylevel = 1
lambda = 1
alpha = 0
sketch_eps = 0.03
refresh_leaf = 1
max_depth = 6
subsample = 1.0
min_child_weight = 1
colsample_bytree = 1.0



# ===============   Task Parameters   =================
# choose logistic regression loss function for binary classification
objective = binary:logistic
base_score = 0.5
seed = 0

# =============== common Parameters ====================
# 0 means do not save any model except the final round model
save_period = 0
# The path of training data
# Is the training data xg format? [1 Yes, 0 No]
xgmat = 0
data = ../train_NN_format/NN_features.txt
label = ../train_NN_format/NNAI.txt
xgdata = ../train_NN_format/NN_features.txt.libsvm
eval_metric = logloss
ascend = 0
# eval: show the train error in each round[0 no]
eval = 1
cv = 5
#  MultiThread
nthread = 4


[xg_tune]
#===============  parameters need to be tuned =================
# the number of round to do boosting
num_round = 500
# maximum depth of a tree
max_depth = 4,6,8,10,15
# max_depth = 8
subsample = 0.7,0.8,0.9,1.0
#subsample = 1.0
min_child_weight = 0.3,0.8,1,2
# min_child_weight = 0.1
colsample_bytree = 0.7,0.8,0.9,1.0
#colsample_bytree = 0.7


