[source_opcode2hash]
#search and convert all the opcode files inside source_dirs recursively 
source_dirs = ../datas/newdownload/combined/201707/

[setup_opcode2hash]
#optional algorithm :map_md5,map_shake, when set as map_md5, the bits is set as 128 default
#when set as map_shake, the bits should be set manually
algorithm = map_shake
#bits should be able to be divied by 8
bits = 512


[dest_opcode2hash]
# destination path for features and labels
dest_features_path = ../datas/newdownload/combined/201707/shake_512_hashvec.dat
dest_labels_path = ../datas/newdownload/combined/201707/shake_512_hashvec.dat.AI


#划分训练测试集的csv文件
[train_test_split]
train = ../datas/Partition/train-test-set-0727/train.csv
test =  ../datas/Partition/train-test-set-0727/test.csv

#features 和 label的路径
[source_path]
source_features_path = ../datas/newdownload/combined/201707/shake_512_hashvec.dat
source_labels_path = ../datas/newdownload/combined/201707/shake_512_hashvec.dat.AI

#是否按照训练集和测试集划分,1为真，表示按照训练集测试集划分
split_train_test = 1
#是否按照正例负例划分
split_malware_normal = 0

#划分好的之后的数据的存放目录
[dest_dir]
dest_dir =  ../datas/Partition/train-test-set-0727/shake_512/


[xg_grid_search]
# ==========   General Parameters, see comment for each definition  ===========
# choose the booster, can be gbtree or gblinear
booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1


# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 0.3
# minimum loss reduction required to make a further partition
gamma = 0.0
# minimum sum of instance weight(hessian) needed in a child
max_delta_step = 0
colsample_bylevel = 1
lambda = 1
alpha = 0
sketch_eps = 0.03
refresh_leaf = 1
max_depth = 6
subsample = 1.0
min_child_weight = 1
colsample_bytree = 1.0


# ===============   Task Parameters   =================
# choose logistic regression loss function for binary classification
objective = binary:logistic
base_score = 0.5
seed = 0

# =============== common Parameters ====================
#share these parameters with hyperopt
# 0 means do not save any model except the final round model
save_period = 0
# The path of training data
# Is the training data xg format? [1 Yes, 0 No]
xgmat = 0
data = ../datas/Partition/train-test-set-0727/shake_512/train_vec.dat
label = ../datas/Partition/train-test-set-0727/shake_512/train_vec.dat.AI
xgdata =  ../datas/Partition/train-test-set-0727/shake_512/train_vec.dat.libsvm

pred_test = 1
dataname = 0707_shake_512
test_data = ../datas/Partition/train-test-set-0727/shake_512/test_vec.dat
test_label = ../datas/Partition/train-test-set-0727/shake_512/test_vec.dat.AI
xgdata_test =  ../datas/Partition/train-test-set-0727/shake_512/test_vec.dat.libsvm
eval_metric = precision
ascend = 1
# eval: show the train error in each round[0 no]
eval = 1
cv = 5
#  MultiThread
nthread = 4


[xg_grid_search_tune]
#===============  parameters need to be tuned =================
# the number of round to do boosting
num_round = 500
# maximum depth of a tree

max_depth = 4,6,8,10,15
subsample = 0.7,0.8,0.9,1.0
min_child_weight = 0.3,0.8,1,2
colsample_bytree = 0.7,0.8,0.9,1.0
colsample_bylevel = 0.6,0.8,1
max_delta_step = 1,3,5
gamma = 0.1,0.3,0.5,0.7

#colsample_bylevel =1
#max_delta_step =0
#gamma = 0
#subsample = 0.7
#max_depth = 15
#min_child_weight = 2.0
#colsample_bytree = 0.7