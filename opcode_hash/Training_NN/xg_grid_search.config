[xg_conf]

booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1

# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 0.3
# minimum loss reduction required to make a further partition
gamma = 0.0
# minimum sum of instance weight(hessian) needed in a child
max_delta_step = 0
colsample_bylevel = 1
lambda = 1
alpha = 0
sketch_eps = 0.03
refresh_leaf = 1
max_depth = 6
subsample = 1.0
min_child_weight = 1
colsample_bytree = 1.0


# ===============   Task Parameters   =================
objective = binary:logistic
base_score = 0.5
seed = 0

# =============== common Parameters ====================
# 0 means do not save any model except the final round model
save_period = 0
eval_metric = logloss
ascend = 0
# eval: show the train error in each round[0 no]
eval = 1
cv = 3
#  MultiThread
nthread = 4

[xg_tune]
#===============  parameters need to be tuned =================
# the number of round to do boosting
num_round = 500
# maximum depth of a tree
max_depth = 4,6,8,10,15
#max_depth = 8
subsample = 0.7,0.8,0.9,1.0
#subsample = 1.0
min_child_weight = 0.3,0.8,1,2
#min_child_weight = 0.1
colsample_bytree = 0.7,0.8,0.9,1.0
#colsample_bytree = 0.7


