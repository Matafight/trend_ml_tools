[source_opcode2hash]
#source_dirs contains many dirs separated by semicolon，label，task，test,class_type are the corresponding value w.r.t the dirs

source_dirs = ../../datas/newdownload/combined/201707/malicious/test
train = ../../datas/Partition/train-test-set-0727/train.csv
test = ../../datas/Partition/train-test-set-0727/test.csv

[setup_opcode2hash]
#optional algorithm :map_md5,map_shake
algorithm = map_md5

#bits should be able to be divied by 8
bits = 48

#split train and test 
split_train_test = 1

#split malware and normal 
split_good_bad = 0

#ignore split_train_test and split_good_bad, generate all the nine files
#merge_all_opcode.csv, train_opcode.csv, test_opcode.csv, normal_opcode.csv, malware_opcode.csv
#train_normal.csv, train_malware.csv, test_normal.csv , test_malware.csv
#1 means ignore 
ignore_split = 1

[dest_opcode2hash]
#suppose we save the converted bits into a csv files
dest_dirs = ./




[xgb_hyopt_xg_conf]
data = ./train_NN_format/NN_features.txt
label = ./train_NN_format/NNAI.txt
# xgmat = 0 mean we need to convert the original format data into libsvm, otherwise we directly read the file specified by xgdata
xgmat = 0
xgdata = ./train_NN_format/NN_features.txt.libsvm
n_fold = 3
#whether save the learned model
save_model = 1
#pred_test = 1 means that we need to input testing data and predict the testing data
pred_test = 1
#Evaluation metrics to be watched in CV. optinal: auc,recall,precision,f1
metrics = auc
# whether the metrics is score or loss,1 means the higher score for this metrics the better performance
if_ascend = 1
booster = gbtree
objective = binary:logistic

[xgb_hyopt_test]
data = ./test_NN_format/NN_features.txt
label = ./test_NN_format/NNAI.txt
xgmat = 0
xgdata = ./test_NN_format/NN_features.txt.libsvm


[xg_grid_search]
# ==========   General Parameters, see comment for each definition  ===========
# choose the booster, can be gbtree or gblinear
booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1

# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 0.3
# minimum loss reduction required to make a further partition
gamma = 0.0
# minimum sum of instance weight(hessian) needed in a child
max_delta_step = 0
colsample_bylevel = 1
lambda = 1
alpha = 0
sketch_eps = 0.03
refresh_leaf = 1
max_depth = 6
subsample = 1.0
min_child_weight = 1
colsample_bytree = 1.0


# ===============   Task Parameters   =================
# choose logistic regression loss function for binary classification
objective = binary:logistic
base_score = 0.5
seed = 0

# =============== common Parameters ====================
#share these parameters with hyperopt
# 0 means do not save any model except the final round model
save_period = 0
# The path of training data
# Is the training data xg format? [1 Yes, 0 No]
#xgmat = 0
#data = ../train_NN_format/NN_features.txt
#label = ../train_NN_format/NNAI.txt
#xgdata =  ../../datas/combined-0707/201707/map_shake_256/train_NN_format/NN_features.txt.libsvm
#xgdata_test = ../../datas/combined-0707/201707/map_shake_256/test_NN_format/NN_features.txt.libsvm

eval_metric = logloss
ascend = 0
# eval: show the train error in each round[0 no]
eval = 1
cv = 5
#  MultiThread
nthread = 4


[xg_grid_search_tune]
#===============  parameters need to be tuned =================
# the number of round to do boosting
num_round = 500
# maximum depth of a tree
max_depth = 4,6,8,10,15
#max_depth = 8
subsample = 0.7,0.8,0.9,1.0
#subsample = 1.0
min_child_weight = 0.3,0.8,1,2
#min_child_weight = 0.1
colsample_bytree = 0.7,0.8,0.9,1.0
#colsample_bytree = 0.7
